# Semantic Hashing

By representing every data item with a compact binary code $b that preserves the similarity information of items, the hashing technique can significantly reduce the memory footprint and increase the search efficiency by working in the binary Hamming space.

## Papers In NLP (text data)

- [Ou, Zijing, et al. "Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval." *the 59th Annual Meeting of the Association for Computational Linguistics (ACL)*.  2021.](https://arxiv.org/pdf/2105.13066.pdf)
  - In **SMUH**, a neighborhood-preserving prior is used, which is reflected in the covariance matrix of the Gaussian distribution. The neighborhood information (i.e., the graph)  is constructed from the raw features (e.g., TF-IDF) of oiringal documents. To enable training, it uses a tree-structured distribution to partially capture the neighborhood information.
- [Zheng, Lin, et al. "Generative Semantic Hashing Enhanced via Boltzmann Machines." *The 58th Annual Meeting of the Association for Computational Linguistics (ACL)*. 2020.](https://arxiv.org/pdf/2006.08858.pdf)
  - **CorrSH** introduces correlations among the bits of hash codes using the Boltzmann-machine distribution. To address the intractability issue of training, The sampling process of the Boltzmann machine distribution can be approximated as:  first sampling $r$​ from the Gaussian distribution $q(r|x)$​, and then sampling $s$​ from the Bernoulli distribution $Bernoulli(\sigma(r))$.
- [Ye, Fanghua, Jarana Manotumruksa, and Emine Yilmaz. "Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling." *EMNLP (Findings)*. Vol. 20. Association for Computational Linguistics (ACL), 2020.](https://discovery.ucl.ac.uk/id/eprint/10117563/1/2020.findings-emnlp.233.pdf)
  - **WISH** foucses on few-bits hashing. It assumes that each text $d$ is generated based on these topic vectors rather than the binary vector $z$. And the only diffence with NASH is adding the orthogonal constraint on topic vectors (i.e., the weights in the specific neural layer).
- [Hansen, Casper, et al. "Unsupervised semantic hashing with pairwise reconstruction." *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)*. 2020.](https://dl.acm.org/doi/pdf/10.1145/3397271.3401220?casa_token=XodiHdfrh48AAAAA:UZGc8PpyjUQGuTWELiXOoXKpMIR_LQUlgZ-7QTULl96drYCpS2SMma-X2atbV_SJayVfMo6iYVNW)
  - Apart from the orginal variaiton lower bound for document $d$, **PairRec** also hope the VAE can regenerate the similar document $d^+$ from the document $d$​
- [Stratos, Karl, and Sam Wiseman. "Learning discrete structured representations by adversarially maximizing mutual information." *International Conference on Machine Learning (ICML)*. PMLR, 2020.](http://proceedings.mlr.press/v119/stratos20a/stratos20a.pdf)
- [Dadaneh, Siamak Zamani, et al. "Pairwise supervised hashing with Bernoulli variational auto-encoder and self-control gradient estimator." *Conference on Uncertainty in Artificial Intelligence (UAI)*. 2020.](http://proceedings.mlr.press/v124/zamani-dadaneh20a/zamani-dadaneh20a.pdf)
  - The network architecture is same with NASH. The only difference is  **ARM-DVAE** utilizes Augment-Reinforce-Merge estimator to estimate the gradient of discrete variables, from which the gradient is unbiased and low-variance.
- [Doan, Khoa D., and Chandan K. Reddy. "Efficient Implicit Unsupervised Text Hashing using Adversarial Autoencoder." *Proceedings of The Web Conference (WWW)*. 2020.](https://dl.acm.org/doi/pdf/10.1145/3366423.3380150?casa_token=OOh8nnBYiMYAAAAA:6VVThvgIadIH25GYO8gLbCFGIddVidby4T7cy_Lce1TTzluhzQ6tf9K-oMHrLBtkqd8YcVvHpB7N)
  - **DABA** employs the denoising adversarial binary autoencoder to generate hashing codes. Unlike the previous method, DABA directly uses the original text and uses the RNN and CNN architecture.
- [Dong, Wei, et al. "Document hashing with mixture-prior generative models." *2019 Conference on Emperical Methods in Natural Language Process (EMNLP)*.2019  ](https://arxiv.org/pdf/1908.11078.pdf)
  - The paper proposes two generative models based on Gaussian mixture prior (**GMSH**) and Bernoulli mixture prior (**BMSH**), which makes the hashing codes more expressive. Of course ,the posterior distribution is either Gaussian Mixture or Bernoulli Mixture.
- [Hansen, Casper, et al. "Unsupervised neural generative semantic hashing." *Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)*. 2019.](https://dl.acm.org/doi/pdf/10.1145/3331184.3331255?casa_token=Wiq6NTQ5PRoAAAAA:jNR9Jx2o_9bYs-4DFokk0IPsNtgwmgYj0eik8mZ750Th67YvRjo-MBuyXglxS8JFfpTA1m9eLAtj)
  - RBSH introduces the pairwise similarity. Specifically, it uses the hash code produced in the paper "Self-Taught Hashing for Fast Similarity Search" as pseudo supervision information. Then it adds the ranking loss based on the pesudo supervision information and hinge loss. The final loss is loss of VAE plus ranking loss.
- [Zhang, Yifei, and Hao Zhu. "Doc2hash: Learning discrete latent variables for documents retrieval." *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), Volume 1 (Long and Short Papers)*. 2019.](https://aclanthology.org/N19-1232.pdf)
  - Instead of using the ST estimator, **Doc2hash** uses the Gumbel-Softmax trick to overcome the inability by applying the re-parameterization trick to discrete latent variables.
- [Chaidaroon, Suthee, et al. "node2hash: Graph aware deep semantic text hashing." *Information Processing & Management* 57.6 (2020): 102143. 2019.](https://www.cse.scu.edu/~yfang/IPM__Node2Hash_Preprint.pdf)
  - **node2hash** thinks that the relationship between documents can also help us infer the semantic information of documents. It first use the sampling method (e.g., BFS, DFS) to sample $K$ neighbors of documents and hopes the hashing code of a document can regenerate its neighbors. This method is suitable for graph data (e.g., Cora, Citeseer).
- [Shen, Dinghan, et al. "NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing." *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)*. 2018.](https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/17415/P18-1190.pdf?sequence=2)
  - **NASH** is the the first semantic hashing architecture that can be trained in an end-to-end manner. Also, it draws connections between the rate-distortion theory.
- [Chaidaroon, Suthee, and Yi Fang. "Variational deep semantic hashing for text documents." *Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)*. 2017.](https://dl.acm.org/doi/pdf/10.1145/3077136.3080816?casa_token=IbZ2gPRQlCwAAAAA:po0Ugy2_X-ixcGpIPIOp_Xwl-R4YAnDBrncR8lAsYr3WZAqoe4HhvS5NaaTS_gf6mYFYVwlbmYi6)
  - **VASH** is the first method using VAE to produce hashing codes of text data. It is still in a two-stage manner.

- [Xu, Jiaming, et al. "Convolutional neural networks for text hashing." *Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI)*. 2015.](https://www.ijcai.org/Proceedings/15/Papers/197.pdf)
- [Wang, Qifan, Dan Zhang, and Luo Si. "Semantic hashing using tags and topic modeling." *Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval (SIGIR)*. 2013.](https://dl.acm.org/doi/pdf/10.1145/2484028.2484037?casa_token=jhEV2K3Rx4UAAAAA:viQmoZtl9xbOo6n5cP0Gh5s3UZg66NGIrbe4IUqkx1da9jHIBGsWT-nMk2rx5ytGpqNVtFpiLHPU)
- [Zhang, Dell, et al. "Self-taught hashing for fast similarity search." *Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR)*. 2010.](https://dl.acm.org/doi/pdf/10.1145/1835449.1835455?casa_token=6YaNFG5jLGYAAAAA:NXJfg_FO7YU9PLPpUiVvpR3QmN0-wGukUNi1r_zNipx25fnL0W5a2Z_YYunhm7uWiDV8whebWF4e)
  - **STH** first obtains the $l$​​​-bit real-vectors through the Laplacian Eigenmap Algorithm and then binarize the real vectors through the thresholds. Second, it using $l$​ binary SVM classifies to optimize the hash function.
- [Salakhutdinov, Ruslan, and Geoffrey Hinton. "Semantic hashing." *International Journal of Approximate Reasoning* 50.7 (2009): 969-978.](https://pdf.sciencedirectassets.com/271876/1-s2.0-S0888613X09X00083/1-s2.0-S0888613X08001813/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG8aCXVzLWVhc3QtMSJHMEUCIQDngTOafc1DBQMCApW5%2BOaXwrnKwhWtnT0E9a8gYOg6dgIgJO9iLaDWnvbw9u6FL%2FWLwYP53sQywEJDXzrk7eTZoUUqgwQIiP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwwNTkwMDM1NDY4NjUiDHgJzKFsJifFwdi%2BNirXA5pewKwwPJJ4ziFYpeWtAKlF%2BJXjlfF%2BC201vRCHdUm7f%2FeOyW5HEtr4FMCFMVK2sKlxP%2FqsLR4pxu%2BllUkqukjvQAzh%2FTWfweSnL6fy9sTeOSxTDN6AcuIUXEROjtHzUkaSTOtmLl0ocDxD8eSkNM7ua%2FYgq5VeyE06ook0Nxd5eLMnFn0pyayBCFqdq6KJTVZarUdZ9hre9Hx%2Bu0VjcI%2FZgG%2FejHPRypUzMktmg2yS0xsklIk6d9%2FSXIjnXH1Vb71qKeTuRWlTb4QZa3baeSn%2FS7yadFiEO79WAHbJyqMDmyhE11xXcMIJ5Q%2BKXxTv5guTWcU6Q3XAaDrdAgBzMB95bmJDPbp4PDoTqcx2sBCtUWf5RS1WqMgrszfblAUQBJdHRqfTwaMbTBWDKwO5KozOQa5ujnw%2Bdc%2FbspvPBHLNwEOsV0Awis%2BHTArWY3KWcFLc2i2GsFPpkT%2FhAK44%2FL%2BJPJvCXhc3mLLGeR2EGd8egiCIrO0CaRYKdnOqYkoWBWVY%2FDkqbvHLY8y3UhCYeEUbXHjtVppfYp7jQc9GIkd82pJV5K4oou4IB5gnITKeZOyyTuPsqsnWp1kvYW5BDooz4MBZnetqN1Vo5AgMheSjDrNNL392nzDm%2Bb2IBjqlAZN1qUHbyDvZ710mOfIpLJX3bZDfk1q78PkQ%2BJz%2Bf%2BikPHaTQ7Aebor7H7ns12xMPrPfU9jx4uKqdi1lBwNHe%2BnIgdjJ%2BFMmJXPu2izOVmI1vrh5H3P%2FJwZPmThY8JSS2%2BCNVBG5fU9%2FLplDThBYlhxVROJQYlByvU9v0RY5iMNtZRdr3CnoOJUczLVx6g0aUL76JaHbbzTSGa9eEFQYrjKha1X9gA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210808T074457Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYRTBHEYMJ%2F20210808%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=a6050b533f1e3b868ea4d3da2273e079c543a09603107916b387ef0fc4fee873&hash=f34025dcbec40b205ff8748916254813ebc4ac5911fa10ee02f0c61893047750&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0888613X08001813&tid=spdf-94b5098b-cdb1-4f52-9d42-2dc55d582d44&sid=6c4327324e19c84eab1a3871e933f54ad575gxrqa&type=client)
- [Weiss, Yair, Antonio Torralba, and Robert Fergus. "Spectral hashing." *Nips*. Vol. 1. No. 2. 2008.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.7577&rep=rep1&type=pdf)
  - **SpH** learns the hashing codes by imposing the balanced and independent constraints.
- [Charikar, Moses S. "Similarity estimation techniques from rounding algorithms." *Proceedings of the thiry-fourth annual ACM symposium on Theory of computing (STOC)*. 2002.](https://dl.acm.org/doi/pdf/10.1145/509907.509965?casa_token=XemET-V4jmkAAAAA:WX8sPzHML3hvUl_ckuXdneNKd9neZXDGHYrZHjPZNcg3BPBGiZM-OjpJs9kzFWP_YnBdBzY2_HpK)
  - The random-projection-based LSH (**RPLSH**)  projects the original real vectors onto $l$​​​​​-bit vectors by a random matrix from standard Gaussian distribution, and then converts them to binary hashing codes through the binarize operation. Its inputs is often the real vectors (e.g., the trained dense embedding), but not the raw data.

## Papers In CV (image data)

- [Qiu, Zexuan, et al. "Unsupervised Hashing with Contrastive Information Bottleneck." *The 30th International Joint Conference on Artificial Intelligence (IJCAI)*. 2021.](https://arxiv.org/pdf/2105.06138.pdf)
  - **CIBHash** first attempts to adapt the contrastive learning to the task of hashing in an end-to-end manner. Further, By viewing the proposed model under the broader IB framework, a more general hashing method is obtained.
- [Shen, Yuming, et al. "Auto-encoding twin-bottleneck hashing." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR*. 2020.](https://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Auto-Encoding_Twin-Bottleneck_Hashing_CVPR_2020_paper.pdf)
  - **TBH** puts forward a variant of Wasserstein Auto-encoder with the batch-wise code-driven adjacency graph to sove the 'static-graph' problem, and thus better guide the image reconstruction process.
- [Shen, Yuming, et al. "Embarrassingly simple binary representation learning." *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCV workshop) *. 2019.](https://openaccess.thecvf.com/content_ICCVW_2019/papers/CEFRL/Shen_Embarrassingly_Simple_Binary_Representation_Learning_ICCVW_2019_paper.pdf)
  - Supervised Hashing. **JMLH** lower-bounds the Information Bottleneck (IB) between data samples and their hashing codes. Its overall loss is equal to cross-entropy loss plus KL divergence.
- [Yang, Erkun, et al. "Distillhash: Unsupervised deep hashing by distilling data pairs." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*. 2019.](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_DistillHash_Unsupervised_Deep_Hashing_by_Distilling_Data_Pairs_CVPR_2019_paper.pdf)
  - **DistillHash** addresses the absence of supervisory signals by distilling data pairs with confident semantic similarity relationships.
- [Shen, Yuming, Li Liu, and Ling Shao. "Unsupervised binary representation learning with deep variational networks." *International Journal of Computer Vision (IJCV)* 127.11 (2019): 1614-1628.](https://link.springer.com/content/pdf/10.1007/s11263-019-01166-4.pdf)
  - **DVB**
- [Su, Shupeng, et al. "Greedy hash: Towards fast optimization for accurate hash coding in cnn." *Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS) *. 2018.](https://proceedings.neurips.cc/paper/2018/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf)
  - **GreedyHash**
- [Song, Jingkuan, et al. "Binary generative adversarial networks for image retrieval." *Thirty-second AAAI conference on artificial intelligence (AAAI)*. 2018.](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17150/15712)
  - **BGAN** designs a loss function which is equal to the weighted sum of a neighbor structure loss, content loss and adversarial loss. Different form TBH, the neighborhood structure is pre-computed.
- [Dizaji, Kamran Ghasedi, et al. "Unsupervised deep generative adversarial hashing network." *Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)*. 2018](https://openaccess.thecvf.com/content_cvpr_2018/papers/Dizaji_Unsupervised_Deep_Generative_CVPR_2018_paper.pdf)
  - **HashGAN**
- [Zieba, Maciej, et al. "Bingan: Learning compact binary descriptors with a regularized gan." *Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS) *. 2018.](https://arxiv.org/pdf/1806.06778.pdf)
  - **BinGAN**
- [Yang, Erkun, et al. "Semantic structure-based unsupervised deep hashing." *Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI)*. 2018.](https://research.ubtrobot.com/file/conferences/Semantic%20Structure-based%20Unsupervised%20Deep%20Hashing.pdf)
  - **SSDH**
- [Dai, Bo, et al. "Stochastic generative hashing." *International Conference on Machine Learning*. PMLR, 2017.](http://proceedings.mlr.press/v70/dai17a/dai17a.pdf)
  - **SGH** is inspired from VAE. The optimized loss is exactly the same with that of **NASH.**
- [Huang, Shanshan, et al. "Unsupervised triplet hashing for fast image retrieval." *Proceedings of the on Thematic Workshops of ACM Multimedia 2017*. 2017.](https://dl.acm.org/doi/pdf/10.1145/3126686.3126773?casa_token=dwTL4ATn8O0AAAAA:esCvYYn3jpL2AbZkjufR0IRBa1s6uMJ3fVfUK2vYwnH2Vc8M6we4YzeC4Mr-Ly3aJ2dMjhzBe_yl)
  - One type of loss in **UTH**is to maximize the discrimination among the image representations through image rotation, which is similar the idea of contrastive learning.
- [Lin, Kevin, et al. "Learning compact binary descriptors with unsupervised deep neural networks." *Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)*. 2016.](https://openaccess.thecvf.com/content_cvpr_2016/papers/Lin_Learning_Compact_Binary_CVPR_2016_paper.pdf)
  - **DeepBit** enforces three criterions on binary codes which are learned at the top layer of the network: minimal loss quantization, evenly distributed codes and rotation invariance. 
- [ Liu, W. , et al. "Discrete graph hashing." *Advances in Neural Information Processing Systems (NIPS)* 4(2014):3419-3427.](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43145.pdf)
  - **DGH**
- [Gong, Yunchao, et al. "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval." *IEEE transactions on pattern analysis and machine intelligence (PAMI)* 35.12 (2012): 2916-2929.](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.4453&rep=rep1&type=pdf)
  - The key point of **ITQ** is that a rotation to the projected vectors can reduce the quantization error. First, the orthogonal matrix R is fixed and we can easily update the binary code B; then the binary code B is fixed, updating R corresponds to the classic Orthogonal Procrustes problem, which has the analytical solution.
- [Liu, W. , et al. "Hashing with Graphs." *Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011* 2011.](https://icml.cc/2011/papers/6_icmlpaper.pdf)
  - **AGH**

## Review Papers

- [Cai, Deng. "A revisit of hashing algorithms for approximate nearest neighbor search." *IEEE Transactions on Knowledge and Data Engineering* (2019).](https://arxiv.org/pdf/1612.07545.pdf)
  - Most surprisingly, the performance of the simple random-projection-based Locality Sensitive Hashing (**LSH**) ranks the first among all the compared baselines when the code length is longer than 128.
- [Wang, Jingdong, et al. "A survey on learning to hash." *IEEE transactions on pattern analysis and machine intelligence (TPAMI)* 40.4 (2017): 769-790.](https://arxiv.org/pdf/1606.00185.pdf)





